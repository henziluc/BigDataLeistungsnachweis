{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Wetterdaten Zürich\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Woher kommen die Daten und in welcher Form liegen sie vor?\n",
    "\n",
    "#### Wetterdaten Zürich und St. Gallen\n",
    "Die Wetterdaten von Zürich und St. Gallen stammen vom Bundesamt für Meteorologie und Kimatologie - MeteoSchweiz. Abrufbar unter folgendem Link: xxx\n",
    "\n",
    "Es handelt sich hierbei um tägliche Messwerte, die als CSV-Dateien im UFT-8 Format vorliegen. Das Datumsfeld trägt den Namen *reference_timestamp* und ist im Format DD.MM.YYYY HH:MM als Zeichenkette gespeichert. Die Dateien umfassen Messwerte aus zwei Wetterstationen: \n",
    "\n",
    "* Zürich Fluntern (SMA)\n",
    "* St. Gallen (STG)\n",
    "\n",
    "#### Luftqualitätsdaten \n",
    "Die Luftqualitätsdaten stammen vom Amt für Abfall, Wasser, Energie udn Luft (AWEL) des Kantons Zürich sowie dem Amt für Umwelt des Kantons St. Gallen. Abrufbar unter folgendem Link: xxx\n",
    "\n",
    "Es handelt sich um tägliche Mittelwerte der Luftschadstoffe, die als CSV-Dataeien im UFT-8 Format vorliegen. Das Datumsfeld trägt den Namen *Datum* und ist im Format JJJ-MM-TT HH:MM+ZZ+ZZ als Zeichenkette gespeichert. Die Dateien umfassen Messwerte für Zwei Luftmessstationen:\n",
    "\n",
    "* Zürich (Zch_Stampfenbachstr)\n",
    "* st. Gallen (StG_St.Leonhard-Str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schritt 1: Wetterdaten Zürich einlesen\n",
    "\n",
    "df_zurich = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .csv(\"data/weather/zurich/ogd-smn_sma_d_historical.csv\")\n",
    "\n",
    "df_zurich.printSchema()\n",
    "df_zurich.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schritt 2: Wetterdaten St. Gallen einlesen\n",
    "\n",
    "df_stgallen = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .csv(\"data/weather/stgallen/ogd-smn_stg_d_historical.csv\")\n",
    "\n",
    "df_stgallen.printSchema()\n",
    "\n",
    "df_stgallen.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schritt 3: Luftqualitätsdaten einlesen\n",
    "\n",
    "df_luft = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"data/air_quality/ugz_ogd_air_d1_*.csv\")\n",
    "\n",
    "df_luft.printSchema()\n",
    "df_luft.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Pivotierung der Luftqualitätsdaten\n",
    "\n",
    "Das ursprüngliche Format der Luftqualitätsdaten lag im \"Long Format\" vor. In dieser Struktur entsprach jede Zeile einen einzelnen Messswert, der durch Datum, Standort sowie Schadstoffparameter bestimmt war. Um die Daten jedoch effizient analysieren und mit den Wetterdaten veknüpfen zu könenn, wird eine Pivotierung vorgenommen. Das Ergebnis ist ein \"Wide Format\", bei dem jeder schadstoff als eigene Spalte dargestellt ist. So enthält jede Zeile alle relevanten Messwerte eines Tages für einen bestimmten Standort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# \"NA\" oder leere Strings als NULL behandeln, dann casten\n",
    "df_luft_clean = df_luft.withColumn(\n",
    "    \"Wert\",\n",
    "    when(col(\"Wert\").isin(\"NA\", \"\", None), None).otherwise(col(\"Wert\").cast(\"double\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_luft_pivot = df_luft_clean.groupBy(\"Datum\", \"Standort\") \\\n",
    "    .pivot(\"Parameter\") \\\n",
    "    .agg(F.first(\"Wert\"))\n",
    "\n",
    "df_luft_pivot.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Datumsformat vereinheitlichen\n",
    "\n",
    "Um die Verarbeitung sowie Verknüpfung der Wetter- und Luftqualitätsdaten zu ermöglichen, wird das Datumsformat in beiden Datsätzen vereinheitlicht. Die ursprüngliche Datums- und Zeitangabe liegen in unterschiedlcihen Formate vor und zwar:\n",
    "\n",
    "* **Wetterdaten**: DD.MM.YYYY HH:mm (reference_timestamp)\n",
    "* **Luftqualitätsdaten**: YYYY-MM-DD'T'HH:MM+ZZ:ZZ (Datum)\n",
    "\n",
    "Durch die Konvertierung in ein einheitliches Datums- und Zeitangabeformat wird sichergestellt, dass beide Quellen korrekt über das Datum gejoint werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Wetterdaten: Datum konvertieren\n",
    "df_zurich = df_zurich.withColumn(\n",
    "    \"date\", to_date(col(\"reference_timestamp\"), \"dd.MM.yyyy HH:mm\")\n",
    ")\n",
    "\n",
    "# Luftqualitätsdaten: Datum konvertieren\n",
    "df_luft_pivot = df_luft_pivot.withColumn(\n",
    "    \"date\", to_date(col(\"Datum\"), \"yyyy-MM-dd'T'HH:mmZ\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Standortbezeichnung vereinheitlichen\n",
    "\n",
    "Um die Wetter- und Luftqualitätsdaten sinnvoll miteinander verknüpfen zu können, muss die Bezeichnung der jeweiligen Standorte gleich sein. In den vorhanden Dateien jedoch gibt es unterschiedliche Bezeichnungen der Standorte, was ein Join erschwert. Beispielsweise steht in den Wetterdaten *SMA* für Zürich und in den Luftqualitätsdaten finden wir *Zch_...* für Zürich. \n",
    "\n",
    "Diese ungleiche Benennung würde einen Join der beiden Datensätze nicht ermöglichen. Somit wird die Standortbezeichnung vereinheitlicht, so dass wir sichergestellen können, dass die Werte für Zürich und St. Gallen korrekt zusammengeführt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "## Luftqualitätsdaten\n",
    "df_air_std = df_luft_pivot.withColumn(\n",
    "    \"location\",\n",
    "    when(col(\"Standort\").startswith(\"Zch_\"), \"Zürich\")\n",
    "    .when(col(\"Standort\").contains(\"St. Gallen\"), \"St. Gallen\")\n",
    "    .otherwise(\"Andere\")\n",
    ")\n",
    "\n",
    "## Wetterdaten\n",
    "df_weather_std = df_zurich.withColumn(\n",
    "    \"location\",\n",
    "    when(col(\"station_abbr\") == \"SMA\", \"Zürich\")\n",
    "    .when(col(\"station_abbr\") == \"STG\", \"St. Gallen\")\n",
    "    .otherwise(\"Andere\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5) Join Wetter- und Luftqualitätsdaten\n",
    "\n",
    "Die Zusammenführung der Wetter- und Luftqualitätsdaten brauchen wir, damit wir später mit einem einheitlichen Datensatz weiter arbeiten können, der alle relevanten Merkmale für eine Modellierung beinhaltet. Die Wetter- und Luftqualitätsdaten sind grundsätzlich örtlich und zeitlich aufeinander abgestimtm worden und können nun miteinander verknüpft werden.\n",
    "\n",
    "Ziel ist es mögliche Wettereinflüsse auf die Luftqualität zu analysieren und so ein prädiktives Modell zu trainieren. Durch den Join stehen uns pro Zeile sowohl meterologische als auch umweltrelevante Parameter pro Tag und Standort zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, try_to_timestamp, col\n",
    "\n",
    "## Wetterdaten\n",
    "df_zurich = df_zurich.withColumn(\n",
    "    \"date\", to_date(col(\"reference_timestamp\"), \"dd.MM.yyyy HH:mm\")\n",
    ")\n",
    "\n",
    "## Luftqualitätsdaten\n",
    "df_luft_pivot = df_luft_pivot.withColumn(\n",
    "    \"date\", to_date(try_to_timestamp(col(\"Datum\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6) Deskriptive Analyse Wetter- und Luftqualitätsdaten nach dem Join\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Zeilen Wetterdaten: 58805\n",
      "Anzahl Zeilen Luftqualitätsdaten: 7308\n"
     ]
    }
   ],
   "source": [
    "## Schritt 1: Anzahl Zeilen in DataFrames vor und nach Join\n",
    "\n",
    "print(\"Anzahl Zeilen Wetterdaten:\", df_zurich.count())\n",
    "print(\"Anzahl Zeilen Luftqualitätsdaten:\", df_luft_pivot.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Schritt 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spalten in df_luft_pivot:\n",
      "['Datum', 'Standort', 'CO', 'NO', 'NO2', 'NOx', 'O3', 'O3_max_h1', 'O3_nb_h1>120', 'PM10', 'PM2.5', 'PN', 'SO2', 'date']\n",
      "\n",
      "Spalten in df_zurich:\n",
      "['station_abbr', 'reference_timestamp', 'tre200d0', 'tre200dx', 'tre200dn', 'tre005d0', 'tre005dx', 'tre005dn', 'ure200d0', 'pva200d0', 'prestad0', 'pp0qffd0', 'ppz850d0', 'ppz700d0', 'pp0qnhd0', 'fkl010d0', 'fkl010d1', 'fu3010d0', 'fu3010d1', 'fkl010d3', 'fu3010d3', 'wcc006d0', 'rre150d0', 'rka150d0', 'htoautd0', 'gre000d0', 'oli000d0', 'olo000d0', 'osr000d0', 'ods000d0', 'sre000d0', 'sremaxdv', 'erefaod0', 'xcd000d0', 'dkl010d0', 'xno000d0', 'xno012d0', 'rreetsd0', 'date']\n"
     ]
    }
   ],
   "source": [
    "print(\"Spalten in df_luft_pivot:\")\n",
    "print(df_luft_pivot.columns)\n",
    "\n",
    "print(\"\\nSpalten in df_zurich:\")\n",
    "print(df_zurich.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
